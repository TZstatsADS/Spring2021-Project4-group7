---
title: "Project 4 Causal Inference"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Propensity Score

Step 0: Set up the environment

```{r}
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("Matching")) install.packages("Matching")
if (!require("glmnet")) install.packages("glmnet")

```

```{r libraries}
library(Matching)
library(glmnet)
library(tidyverse)
```

Step 1: Load Data

```{r}
lowDim <- read.csv("../data/lowDim_dataset.csv")
highDim <- read.csv("../data/highDim_dataset.csv")
```

```{r}
head(lowDim,5)
head(highDim,5)
```

```{r}
# Split into A, x and y
#highDim_dataset
# highA<-highDim$A
# highY<-highDim$Y
# highX<-highDim%>% select(-Y, -A) %>% as.matrix
# 
# #lowDim_dataset
# lowA<-lowDim$A
# lowY<-lowDim$Y
# lowX<-lowDim%>% select(-Y, -A) %>% as.matrix
```

step 2: L2 Penalized Logistic Regression

To avoid overfitting of the logistic regression model, we introduce regularization term to decrease the model variance in the loss function Q


```{r}
propensity_score <- function(data){
  set.seed(0)
  start_time <- Sys.time()
  A<-data$A
  Y<-data$Y
  X<-data%>% select(-Y, -A) %>% as.matrix
  
  
  glm <- cv.glmnet(X, A, family = "binomial", alpha = 0)
  
  p_score <- predict(glm$glmnet.fit, 
                      s = glm$lambda.min, 
                      newx = as.matrix(X),
                      type = "response")
  
  end_time <- Sys.time()
  running_time = end_time - start_time
  #cat("Time for Propensity Matching Low_Dim is:", running_time, "seconds.")
  return(p_score)
}
```


```{r}
# propensity_score(lowDim)
# propensity_score(highDim)
```




# Stratification

$$\hat \Delta_S = \sum_{j=1}^K \frac{N_j}{N} \{N_{1j}^{-1} \sum_{i=1}^N T_i Y_i I(\hat e_i \in \hat Q_j) - N_{0j}^{-1} \sum_{i=1}^N (1-T_i) Y_i I(\hat e_i \in \hat Q_j)\}$$

## L2 Logistic Regression Propensity Score

```{r}
ps_high = propensity_score(highDim)
ps_low = propensity_score(lowDim)
```

## Estimate Average Treatment Effect (ATE)

```{r}
K = 5 # number of strata
q = seq(0, 1, by = 1/K) # sample quantile
```


```{r}
stratification = function(df, ps){
  tm_start = Sys.time()
  # split into K strata
  stratum = rep(NA, length(q))
  for (i in 1:length(q)){
    stratum[i] = quantile(ps, q[i])
  }
  # ATE
  ate_strata = rep(NA, K)
  for (j in 1:K){
    # select observations whose propensity score is within (q_{j-1},q_j]
    curr.obs = df[which(stratum[j] < ps & ps <= stratum[j+1]),]
    
    # subset/select treatment and control groups
    treatment_strata = curr.obs[curr.obs$A == 1,]
    control_strata = curr.obs[curr.obs$A == 0,]
    
    # calculate sum within stratum
    treatment_sum = sum(treatment_strata$A * treatment_strata$Y) / nrow(treatment_strata)
    control_sum = sum((1 - control_strata$A) * control_strata$Y) / nrow(control_strata)
    ate_strata[j] = (nrow(curr.obs)/nrow(df)) * (treatment_sum - control_sum)
  }
  tm_end = Sys.time()
  cat("The estimated ATE is", sum(ate_strata), "\n")
  cat("Run time is", (tm_end - tm_start), "s")
  return(list(sum(ate_strata), (tm_end - tm_start)))
}
```


### High Dimension Data Set

```{r}
strata_high = stratification(highDim, ps_high)
```

### Low Dimension Data Set

```{r}
strata_low = stratification(lowDim, ps_low)
```


## Performance

* True ATE for High Dimension Data Set: -54.8558

* True ATE for Low Dimension Data Set: 2.0901

```{r}
true_ate_high = -54.8558
true_ate_low = 2.0901
cat("Performance for High Dimension Data is", abs(true_ate_high - strata_high[[1]]), "\n")
cat("Performance for Low Dimension Data is", abs(true_ate_low - strata_low[[1]]))
```

```{r}
res = matrix(rep(NA,4), ncol = 2)
colnames(res) = c("Performance", "Computational Efficiency")
rownames(res) = c("High Dimension", "Low Dimension")
res[1,1] = abs(true_ate_high - strata_high[[1]])
res[1,2] = abs(true_ate_low - strata_low[[1]])
res[2,1] = strata_high[[2]]
res[2,2] = strata_low[[2]]
res
```


# Regression Estimate with no need of propensity score

### This algorithm builds regression model fit to control groups and treatment groups, and then makes predictions based on the model in order to calculate ATE. 

## Load Data

```{r}
lowDim_df <- read.csv('../data/lowDim_dataset.csv')
highDim_df <- read.csv('../data/highDim_dataset.csv')
```

## Algorithm

```{r}
Regression_Estimation <- function(df){
  
  start_time <- Sys.time()
  
  # regression model for control groups (A=0)
  model0 <- glm(formula=Y~., data=subset(df[which(df$A==0),],select=-c(A)))
  # regression model for treatment groups (A=1)
  model1 <- glm(formula=Y~., data=subset(df[which(df$A==1),],select=-c(A)))
  
  X = subset(df, select=-c(Y,A)) #input data for prediction
  
  #prediction using model0
  Y0 <- predict(model0, newdata=X)
  #prediction using model1
  Y1 <- predict(model1, newdata=X)
  
  # calculate ATE 
  ATE <- mean(Y1-Y0)
  
  # calculate running time
  end_time <- Sys.time()
  running_time <- end_time - start_time
  
  return (list(ATE=ATE, running_time=running_time))
}
```

## summarize

* ATE
```{r}
calc_ate_low <- Regression_Estimation(lowDim_df)$ATE
calc_ate_high <- Regression_Estimation(highDim_df)$ATE
cat("ATE for Low Dimension Data is", calc_ate_low, "\n")
cat("ATE for High Dimension Data is", calc_ate_high)
```

* Running Time 
```{r}
running_time_low <- Regression_Estimation(lowDim_df)$running_time
running_time_high <- Regression_Estimation(highDim_df)$running_time
cat("Running Time for Low Dimension Data is", running_time_low, "sec.","\n")
cat("Running Time for High Dimension Data is", running_time_high, "sec.")
```

* Performance
```{r}
true_ate_low = 2.0901
true_ate_high = -54.8558
performance_low <- abs(true_ate_low - calc_ate_low)
performance_high <- abs(true_ate_high - calc_ate_high)
cat("Performance for Low Dimension Data is", performance_low, "\n" )
cat("Performance for High Dimension Data is", performance_high)
```

* Result
```{r}
RE <- matrix(c(calc_ate_high,calc_ate_low,running_time_high,running_time_low,performance_high,performance_low), ncol = 3, nrow=2)
colnames(RE) = c("ATE","Computational Efficiency","Performance")
rownames(RE) = c("High Dimension", "Low Dimension")
RE
```

# Weighted Regression

```{r prep data reg estimate}
df_ld <- lowDim %>% mutate(A = factor(A))
df_hd <- highDim %>% mutate(A = factor(A))
```

```{r}
#assign variables for data frame
X_low <- df_ld %>% select(-Y, -A) %>% as.matrix
A_low <- df_ld %>% select(A) %>% as.matrix
X_high <- df_hd %>% select(-Y, -A) %>% as.matrix
A_high <- df_hd %>% select(A) %>% as.matrix

#running cv with L2
cv_L2_low <- cv.glmnet(X_low, A_low, family = "binomial", alpha = 0)
cv_L2_high <- cv.glmnet(X_high, A_high, family = "binomial", alpha = 0)
```


```{r Weighted_Regression fucntion}

Weighted_Regression <- function(X,A,df,cv, threshold){
  start_time <- Sys.time()
  #L2 to get propensity score -low 
  L2 <- glmnet(X, A, family = "binomial",
                 alpha = 0, lambda = cv$lambda.min)
  #calculate propensity score
  propensity_score <- predict(L2, X, type = "response")
  
  # Finding weights
  t<- as.numeric(A)
  wt<- t/(propensity_score) + (1-t)/(1-propensity_score)
  
  # Estimate linear regression
  Y <- df$Y
  model<- lm(Y~., data = df)
  feature_z <- summary(model)$coef[,4][-c(1:2)]<threshold
  Z <- as.data.frame(cbind(A,X[,feature_z]))
  Z<-sapply(Z, as.numeric)
  
  #Weighted Regression
  weighted_reg <- lm(Y ~ Z, weights = wt)
  
  #coef of T is an estimate for ATE
  ATE<- coef(weighted_reg)[2]
  end_time <- Sys.time()
  running_time <- end_time - start_time
  cat("The estimated ATE is", ATE, "\n")
  cat("Run time is", running_time , "s")
  return (list(ATE=ATE, running_time=running_time))
  
}
  
```

# different thresholds for best ATE
```{r}
thresholds <- c(0.1, 0.05, 0.02, 0.01, 0.005)

ate_scores_low <- c()
ate_scores_high <- c()

true_ate_high = -54.8558
true_ate_low = 2.0901

for (t in thresholds) {
  wreg_low = Weighted_Regression(X_low, A_low, df_ld, cv_L2_low, t)
  wreg_high = Weighted_Regression(X_high, A_high, df_hd, cv_L2_high, t)
  diff_low = abs(true_ate_low - wreg_low[[1]])
  diff_high = abs(true_ate_high - wreg_high[[1]])
  append(ate_scores_low, diff_low)
  append(ate_scores_high, diff_high)
}
```

Set threshold to the optimal value:
```{r}
opt_low <- which.min(ate_scores_low)
opt_high <- which.min(ate_scores_high)
threshold_low <- thresholds[opt_low]
threshold_high <- thresholds[opt_high]
```

```{r}
Weighted_Regression_low = Weighted_Regression(X_low,A_low,df_ld,cv_L2_low, threshold_low)
```

```{r}
Weighted_Regression_high = Weighted_Regression(X_high,A_high,df_hd,cv_L2_high, threshold_high)
```

```{r}
true_ate_high = -54.8558
true_ate_low = 2.0901
cat("Performance for High Dimension Data is", abs(true_ate_high - Weighted_Regression_high[[1]]), "\n")
cat("Performance for Low Dimension Data is", abs(true_ate_low -Weighted_Regression_low[[1]]))
```

```{r}
res = matrix(rep(NA,4), ncol = 2)
colnames(res) = c("Performance", "Computational Efficiency")
rownames(res) = c("High Dimension", "Low Dimension")
res[1,1] = abs(true_ate_high - Weighted_Regression_high[[1]])
res[1,2] = abs(true_ate_low - Weighted_Regression_low[[1]])
res[2,1] = Weighted_Regression_high[[2]]
res[2,2] = Weighted_Regression_low[[2]]
res
```